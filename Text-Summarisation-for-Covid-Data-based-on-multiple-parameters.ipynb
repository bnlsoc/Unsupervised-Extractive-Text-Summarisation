{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Some basic points about NLP</H4>\n",
    "<br>\n",
    "Natural Language Processing or NLP is basically a sub-field of artificial intelligence in which we make computer system learn, analyse and generate natural language\n",
    "<hr style=\"height: 2pt;background-color: red\">\n",
    "NLP : consists of NLU and NLG<br>\n",
    "       NLU - Natural Language Understanding\n",
    "       NLG - Natural Language Generation\n",
    "<h5>5 different phases of NLU and NLG</h5>\n",
    "<ol>\n",
    "<li>Lexical Processing:- tokenisation. morphological analysis, processing on individual words</li>\n",
    "<li>Syntactic Processing :- Internal representation of the text, example a parse tree representation.</li>\n",
    "<li>Semantic Processing :- Clarifying the meaning of the word, meaning of words may be different in different context, for example, Federal Bank, bank of a river</li>\n",
    "<li>Disposal/Pragmatic Processing:- Former deals with emotions (like text to speech) and Pragmatic deals with stories (eg John is a monk. He goes to Church Daily. He is a Catholic.)</li>\n",
    "</ol>\n",
    "<hr style=\"height: 2pt;background-color: blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Summmarisation System</h1>\n",
    "<hr style=\"height: 2pt;background-color: green\">\n",
    "Condensing a longer document into a short concise document without losing the core information\n",
    "<br>\n",
    "Based on input, it can be a sinlge document or multi-document summary\n",
    "<br>Based on the Purpose: Like some documents are generic or some from one domain (like summarising covid-19 dataset is domain)\n",
    "<br>Query Based: User asks questions.\n",
    "<h6>Extractive (just retains main sentences) and Abstractive (writing the summary in own words)</h6>\n",
    "\n",
    "\n",
    "<It's assumed you are familiar with supervised and unsupervised learning>\n",
    "<hr style=\"height: 2pt;background-color: red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Text summariation by taking into account various features</h4>\n",
    "<br>It involves the following steps\n",
    "\n",
    "<ol>\n",
    "    <li>Pre Processing\n",
    "        <ul>\n",
    "            <li>Sentence Segmentation</li>\n",
    "            <li>Tokenization</li>\n",
    "            <li>Stop-Words Removal</li>\n",
    "            <li>Stemming</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    <li>Feature Extraction\n",
    "        <ul>\n",
    "            <li>Word Score</li>\n",
    "            <li>Sentence Score</li>\n",
    "        </ul>\n",
    " </ol>\n",
    " \n",
    " <h6>Quotes are an important part of summary </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Libraries\n",
    "#NLTK-natural language toolkit for natural language processing\n",
    "#CORPUS- Collection of Documents, eg Wall Street Journal CORPUS\n",
    "#using stop-words CORPUS, stop-words are words like of, are, is etc, \n",
    "#which occur more frequently and have no semantic meaning\n",
    "#We need to tokenize the words because we need to compute the frequency of each word\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "#Stemmer goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import documents\n",
    "f = open(('./trial_covid_dataset.txt'),\"r\")\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, we have stored the document's text into text variable\n",
    "#Preprocessing the data : Very Important to avoid overfit or underfit\n",
    "\n",
    "#Step-1 We tokenize each sentence\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(text)\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "#Step-2 We convert to lower case\n",
    "word_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "#Step-3 remove stopwords\n",
    "stopWords = list(set(stopwords.words('english'))) #getting all stopwords of English and storing in StopWords\n",
    "word_tokens_refined = [word for word in word_tokens_lower if word not in stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stem = []\n",
    "for word in word_tokens_refined:\n",
    "    stem.append(ps.stem(word))\n",
    "    #storing all the variants of the word\n",
    "word_tokens_refined=stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The goal of a stem is to remove as much variance as possible so as to fit in different cases</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #It hasn't been run yet\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "# stem = []\n",
    "# wnl = WordNetLemmatizer()\n",
    "# for word in word_tokens_refined:\n",
    "#     if wnl.lemmatize(word).endswith('e'):\n",
    "#         stem.append(wnl.lemmatize(word))\n",
    "#     else:\n",
    "#         stem.append(ps.stem(word))\n",
    "#     #storing all the variants of the word\n",
    "# word_tokens_refined=stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Finding the number of Proper Nouns in each sentence</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proper noun\n",
    "proper_noun = {} #empty dict\n",
    "from nltk.tag import pos_tag #part of speech tag\n",
    "#for each sentence we tag each sentence\n",
    "for sentence in sent_tokens:\n",
    "    proper_noun[sentence]=0 #initialising the dictionary value to zero\n",
    "    tagged_sentence=pos_tag(sentence.split()) #getting a tagged_sentence list for tagging the words\n",
    "    #in this sentence\n",
    "    proper_nouns_in_this_sentence = [word for word,pos in tagged_sentence if pos==\"NNP\"] #you know how to write this one-liner in multiple ways\n",
    "    proper_noun[sentence]=len(proper_nouns_in_this_sentence)\n",
    "#So in proper noun,we get the score of each sentence based on proper nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Normalising the number of proper nouns in each sentence</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise the proper_noun DICTIONARY\n",
    "maximum = max(proper_noun.values())\n",
    "for key in proper_noun:\n",
    "    try:\n",
    "        proper_noun[key] = proper_noun[key]/maximum\n",
    "    except ZeroDivisionError:\n",
    "        x = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Based on Number of Cue Phrases in Each Sentence</h4>\n",
    "<hr>\n",
    "<h6>Getting the list of cue_phrases </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cue Phrases - Highly important words and signify the importance of a sentence, signify is also a cue-phrase\n",
    "#Getting the *list* of qphrases\n",
    "qphrases = []\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "a = pd.read_csv('./cue_phrases.csv')\n",
    "#ideally get the list from tech retireval conference society\n",
    "#This list is incomplete. it won't yield a good summary\n",
    "b = np.array(a)\n",
    "for i in range(0,len(b)):\n",
    "    qphrases.append(b[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Finding the Number of Cue Phrases in Each Sentence</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cue_phrase_dict = {}\n",
    "for sentence in sent_tokens:\n",
    "    cue_phrase_dict[sentence]=0\n",
    "    word_tokens_in_this_sentence=nltk.word_tokenize(sentence)\n",
    "    for word in word_tokens_in_this_sentence:\n",
    "        if word.lower() in qphrases:\n",
    "            cue_phrase_dict[sentence]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Normalising the number of cue_phrases in each sentence</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normaalising the values of cue_words\n",
    "maximum = max(cue_phrase_dict.values())\n",
    "for key in cue_phrase_dict:\n",
    "    try:\n",
    "        cue_phrase_dict[key] = cue_phrase_dict[key]/maximum\n",
    "    except ZeroDivisionError:\n",
    "        x = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>So far, we have discussed proper nouns and cue_words. Now coming to numerical digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = {}\n",
    "for sentence in sent_tokens:\n",
    "    numeric_data[sentence]=0\n",
    "    word_tokens_in_this_sentence = nltk.word_tokenize(sentence)\n",
    "    for word in word_tokens_in_this_sentence:\n",
    "        if word.isdigit():\n",
    "            numeric_data[sentence]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Normalising the number of numeric data in each sentence</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = max(numeric_data.values())\n",
    "for key in numeric_data:\n",
    "    try:\n",
    "        numeric_data[key] = numeric_data[key]/maximum\n",
    "    except ZeroDivisionError:\n",
    "        x = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Similarly, we can find uppercase, formatting, like word.isdigit()\n",
    "#Word_Frequency_can also be added! Try it once\n",
    "<br>\n",
    "    <hr style=\"height: 2px;\">\n",
    "<b><u>Similarly, in the following code bloks, we take into account sentence length, sentence ordering, uppercase sentence,heading matches, frequency</u> </b>\n",
    "    \n",
    "<hr style=\"height: 2px\">\n",
    "<h5>Formula for sentence length</h5>\n",
    "<img src=\"./sent_length.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_length\n",
    "sentence_length = {}\n",
    "for sentence in sent_tokens:\n",
    "    word_tokens_in_this_sentence=nltk.word_tokenize(sentence)\n",
    "    if len(word_tokens_in_this_sentence) in range(0,10):\n",
    "        sentence_length[sentence]=1-0.058*(10-len(word_tokens_in_this_sentence))\n",
    "    elif len(word_tokens_in_this_sentence) in range(10,20):\n",
    "        sentence_length[sentence]=1\n",
    "    else:\n",
    "        sentence_length[sentence] = 1-0.05*(len(word_tokens_in_this_sentence)-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height: 2px\">\n",
    "<h5>Formula for sentence Position</h5>\n",
    "<img src=\"./sent_pos.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_position\n",
    "sentence_position={}\n",
    "n = 1\n",
    "N = len(sent_tokens)\n",
    "for sentence in sent_tokens:\n",
    "    a =1/n\n",
    "    b = 1/(N-n+1)\n",
    "    sentence_position[sentence]=max(a,b)\n",
    "    n = n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word matches with Heading\n",
    "head_match = {} #empty dictionary\n",
    "heading=sent_tokens[0] #first sentence as the heading\n",
    "\n",
    "#Now for heading matching\n",
    "\n",
    "for sentence in sent_tokens:\n",
    "    head_match[sentence]=0 #intitally heading match with that sentence is zero\n",
    "    word_tokens_in_this_sentence = nltk.word_tokenize(sentence)\n",
    "    for word in word_tokens_in_this_sentence:\n",
    "        if word not in stopWords:#if the word is not a stopword\n",
    "            word = ps.stem(word) #stemming the word\n",
    "            if word in ps.stem(heading):#check if its in heading\n",
    "                head_match[sentence]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalising the heading matches\n",
    "maximum = max(head_match.values())\n",
    "for key in head_match:\n",
    "    try:\n",
    "       head_match[key] = head_match[key]/maximum\n",
    "    except ZeroDivisionError:\n",
    "        x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_case={}\n",
    "for sentence in sent_tokens:\n",
    "    upper_case[sentence] = 0\n",
    "    word_tokens_in_this_sentence = nltk.word_tokenize(sentence)\n",
    "    for word in word_tokens_in_this_sentence:\n",
    "        if word.isupper():\n",
    "            upper_case[sentence] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalising the upper case dictionary values\n",
    "\n",
    "maximum = max(upper_case.values())\n",
    "for key in head_match:\n",
    "    try:\n",
    "       upper_case[key] = head_match[key]/maximum\n",
    "    except ZeroDivisionError:\n",
    "        x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#frequency table\n",
    "freqTable = {}\n",
    "for word in word_tokens_refined:    \n",
    "    if word in freqTable:         \n",
    "        freqTable[word] += 1    \n",
    "    else:         \n",
    "        freqTable[word] = 1\n",
    "        \n",
    "\n",
    "for k in freqTable.keys():\n",
    "    freqTable[k]= math.log10(1+freqTable[k])\n",
    "    \n",
    "\n",
    "#computing word frequency for each sentence\n",
    "word_frequency={} #empty dictionary for word_frequency\n",
    "for sentence in sent_tokens:\n",
    "    word_frequency[sentence]=0\n",
    "    unstemmed_word_tokens_in_this_sentence=nltk.word_tokenize(sentence)\n",
    "    stemmed_Word_tokens_in_this_sentence=[]\n",
    "    for word in unstemmed_word_tokens_in_this_sentence:\n",
    "        stemmed_Word_tokens_in_this_sentence.append(ps.stem(word))\n",
    "        #so we have got stemmed words for this sentence\n",
    "    for word,freq in freqTable.items():\n",
    "        if word in stemmed_Word_tokens_in_this_sentence:\n",
    "            #if thw word is in the frequency table and in stemmd word tokens of this sentence, we add\n",
    "            #the frequency of the word to this sentence\n",
    "            word_frequency[sentence]+=freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalising the word_frequency\n",
    "\n",
    "maximum = max(word_frequency.values())\n",
    "for key in head_match:\n",
    "    try:\n",
    "       word_frequency[key] = word_frequency[key]/maximum\n",
    "    except ZeroDivisionError:\n",
    "        x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score={}\n",
    "for k in sent_tokens:\n",
    "    total_score[k]=cue_phrase_dict[k]+numeric_data[k]+sentence_length[k]+sentence_position[k]+word_frequency[k]+upper_case[k]+proper_noun[k]+head_match[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success from two leading coronavirus vaccine programs likely means other frontrunners will also show strong protection against COVID-19, Bill Gates said Tuesday.The fact that two coronavirus vaccines recently showed strong protection against COVID-19 bodes well for other leading programs led by AstraZeneca, Novavax, and Johnson & Johnson, Bill Gates said Tuesday.The billionaire Microsoft founder and philanthropist said it will be easier to boost manufacturing and distribute these other shots to the entire world, particularly developing nations.The vaccine space has seen a flurry of good news in recent days, marked by overwhelming success in late-stage trials by both Pfizer and Moderna.\"With the very good news from Pfizer and Moderna, we think it's now likely that AstraZeneca, Novavax, and Johnson & Johnson will also likely show very strong efficacy,\" Gates told journalist Andrew Ross Sorkin.The scientific success has turned the top challenges surrounding a COVID-19 vaccine to the manufacturing and distribution front.Gates noted that the world will be supply constrained for 2021, but these additional vaccines will prove valuable on that front.\n"
     ]
    }
   ],
   "source": [
    "#Now retaining the important information using average\n",
    "import numpy as np\n",
    "average = np.mean(list(total_score.values()))\n",
    "\n",
    "summary = ''\n",
    "\n",
    "for sentence in sent_tokens:\n",
    "    if total_score[sentence]>average:\n",
    "        summary = summary+sentence\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.4,
   "position": {
    "height": "244px",
    "left": "695.8px",
    "right": "20px",
    "top": "6px",
    "width": "327px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
